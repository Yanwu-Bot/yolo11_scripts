import os
import cv2
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from ultralytics import YOLO
from matplotlib import rcParams
import warnings
warnings.filterwarnings('ignore')

# å­—ä½“è®¾ç½®
rcParams['font.family'] = 'SimHei'

# ============================ é…ç½®é¡¹ ============================
# è®¾å¤‡é…ç½®
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ä½¿ç”¨è®¾å¤‡: {DEVICE}")

# æ¨¡å‹è·¯å¾„ï¼ˆè®­ç»ƒå¥½çš„æ¨¡å‹ï¼‰
MODEL_PATH = "model/best_model.pth"

# æ¨¡å‹å‚æ•°ï¼ˆè¿™äº›åº”è¯¥ä¸è®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´ï¼Œä»checkpointä¸­è¯»å–ï¼‰
# æ³¨æ„ï¼šæˆ‘ä»¬ä¸åœ¨è¿™é‡Œç¡¬ç¼–ç ï¼Œè€Œæ˜¯ä»checkpointä¸­è¯»å–

# æ—¶åºçª—å£å‚æ•°
WINDOW_SIZE = 12         # æ—¶é—´çª—å£å¤§å°ï¼ˆå¿…é¡»ä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰

# å…³é”®ç‚¹æå–å‚æ•°
CONF_THRESHOLD = 0.5     # YOLOå…³é”®ç‚¹ç½®ä¿¡åº¦é˜ˆå€¼

# é¢„æµ‹é˜ˆå€¼
PRED_THRESHOLD = 0.5     # æ¦‚ç‡>é˜ˆå€¼åˆ¤å®šä¸ºå¼‚å¸¸

# ==============================================================================

# -------------------------- 1. ç›´æ¥åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹ --------------------------
def load_trained_model(model_path):
    """ç›´æ¥åŠ è½½è®­ç»ƒå¥½çš„STGCNæ¨¡å‹"""
    if not os.path.exists(model_path):
        print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ï¼š{model_path}")
        return None
    
    # ç›´æ¥åŠ è½½æ•´ä¸ªcheckpoint
    checkpoint = torch.load(model_path, map_location=DEVICE)
    
    # æ£€æŸ¥checkpointçš„å†…å®¹
    print(f"ğŸ“‹ CheckpointåŒ…å«çš„é”®ï¼š{list(checkpoint.keys())}")
    
    if 'model_state_dict' in checkpoint:
        # ä»checkpointä¸­åŠ è½½æ¨¡å‹
        print("âœ… ä»checkpointä¸­åŠ è½½æ¨¡å‹çŠ¶æ€å­—å…¸")
        
        # æˆ‘ä»¬éœ€è¦çŸ¥é“æ¨¡å‹çš„é…ç½®ï¼Œå¯ä»¥ä»checkpointä¸­è¯»å–æˆ–ä½¿ç”¨é»˜è®¤å€¼
        model_config = checkpoint.get('model_config', {
            'num_classes': 2,
            'in_channels': 2,
            't_kernel_size': 3,
            'hop_size': 2
        })
        
        # æ‰“å°æ¨¡å‹é…ç½®
        print(f"ğŸ“Š æ¨¡å‹é…ç½®ï¼š{model_config}")
        
        # åŠ¨æ€å¯¼å…¥è®­ç»ƒæ—¶ä½¿ç”¨çš„æ¨¡å‹å®šä¹‰
        # æ³¨æ„ï¼šè¿™é‡Œå‡è®¾ä½ çš„è®­ç»ƒä»£ç åœ¨åŒä¸€ç›®å½•ä¸‹
        try:
            # æ–¹æ³•1ï¼šç›´æ¥å¯¼å…¥è®­ç»ƒä»£ç ä¸­çš„æ¨¡å‹ç±»
            from ST_GCN import COCO_ST_GCN  # éœ€è¦æ”¹æˆä½ çš„è®­ç»ƒæ–‡ä»¶å
            print("âœ… ä»è®­ç»ƒæ–‡ä»¶å¯¼å…¥æ¨¡å‹ç±»")
        except ImportError:
            # æ–¹æ³•2ï¼šä½¿ç”¨ä¸€ä¸ªç®€åŒ–çš„æ¨¡å‹å®šä¹‰ï¼ˆå¦‚æœçŸ¥é“ç¡®åˆ‡ç»“æ„ï¼‰
            print("âš ï¸ æ— æ³•å¯¼å…¥è®­ç»ƒæ¨¡å‹ï¼Œä½¿ç”¨ç®€åŒ–æ¨¡å‹å®šä¹‰")
            
            # è¿™é‡Œéœ€è¦å®šä¹‰å’Œè®­ç»ƒæ—¶å®Œå…¨ä¸€æ ·çš„æ¨¡å‹ç»“æ„
            # è¿™éƒ¨åˆ†åº”è¯¥ä»ä½ çš„è®­ç»ƒä»£ç ä¸­å¤åˆ¶è¿‡æ¥
            class COCOGraph():
                def __init__(self, hop_size=2):
                    self.num_node = 17
                    self.hop_size = hop_size
                    self.get_edge()
                    self.hop_dis = self.get_hop_distance(self.num_node, self.edge, hop_size=hop_size)
                    self.get_adjacency()
                
                def get_edge(self):
                    self_link = [(i, i) for i in range(self.num_node)]
                    neighbor_base = [
                        (0, 1), (0, 2), (1, 3), (2, 4),
                        (5, 6), (5, 7), (7, 9), (6, 8), (8, 10),
                        (11, 12), (5, 11), (6, 12),
                        (11, 13), (13, 15), (12, 14), (14, 16)
                    ]
                    neighbor_link = [(i, j) for (i, j) in neighbor_base]
                    self.edge = self_link + neighbor_link
                
                def get_adjacency(self):
                    valid_hop = range(0, self.hop_size + 1, 1)
                    adjacency = np.zeros((self.num_node, self.num_node))
                    for hop in valid_hop:
                        adjacency[self.hop_dis == hop] = 1
                    normalize_adjacency = self.normalize_digraph(adjacency)
                    A = np.zeros((len(valid_hop), self.num_node, self.num_node))
                    for i, hop in enumerate(valid_hop):
                        A[i][self.hop_dis == hop] = normalize_adjacency[self.hop_dis == hop]
                    self.A = A
                
                def get_hop_distance(self, num_node, edge, hop_size):
                    A = np.zeros((num_node, num_node))
                    for i, j in edge:
                        A[j, i] = 1
                        A[i, j] = 1
                    hop_dis = np.zeros((num_node, num_node)) + np.inf
                    transfer_mat = [np.linalg.matrix_power(A, d) for d in range(hop_size + 1)]
                    arrive_mat = (np.stack(transfer_mat) > 0)
                    for d in range(hop_size, -1, -1):
                        hop_dis[arrive_mat[d]] = d
                    return hop_dis
                
                def normalize_digraph(self, A):
                    Dl = np.sum(A, 0)
                    num_node = A.shape[0]
                    Dn = np.zeros((num_node, num_node))
                    for i in range(num_node):
                        if Dl[i] > 0:
                            Dn[i, i] = Dl[i]**(-1)
                    DAD = np.dot(A, Dn)
                    return DAD
            
            class SpatialGraphConvolution(nn.Module):
                def __init__(self, in_channels, out_channels, s_kernel_size):
                    super().__init__()
                    self.s_kernel_size = s_kernel_size
                    self.conv = nn.Conv2d(in_channels=in_channels,
                                        out_channels=out_channels * s_kernel_size,
                                        kernel_size=1)
                
                def forward(self, x, A):
                    x = self.conv(x)
                    n, kc, t, v = x.size()
                    x = x.view(n, self.s_kernel_size, kc//self.s_kernel_size, t, v)
                    x = torch.einsum('nkctv,kvw->nctw', (x, A))
                    return x.contiguous()
            
            class STGC_block(nn.Module):
                def __init__(self, in_channels, out_channels, stride, t_kernel_size, A_size, dropout=0.5):
                    super().__init__()
                    self.sgc = SpatialGraphConvolution(in_channels=in_channels,
                                                    out_channels=out_channels,
                                                    s_kernel_size=A_size[0])
                    self.M = nn.Parameter(torch.ones(A_size))
                    self.tgc = nn.Sequential(
                        nn.BatchNorm2d(out_channels),
                        nn.ReLU(),
                        nn.Dropout(dropout),
                        nn.Conv2d(out_channels, out_channels,
                                (t_kernel_size, 1), (stride, 1),
                                ((t_kernel_size - 1) // 2, 0)),
                        nn.BatchNorm2d(out_channels),
                        nn.ReLU())
                
                def forward(self, x, A):
                    x = self.tgc(self.sgc(x, A * self.M))
                    return x
            
            class COCO_ST_GCN(nn.Module):
                def __init__(self, num_classes=2, in_channels=2, t_kernel_size=9, hop_size=2):
                    super().__init__()
                    from ST_GCN import COCOGraph  # å‡è®¾å›¾å®šä¹‰åœ¨å•ç‹¬æ–‡ä»¶
                    graph = COCOGraph(hop_size)
                    A = torch.tensor(graph.A, dtype=torch.float32, requires_grad=False)
                    self.register_buffer('A', A)
                    A_size = A.size()
                    
                    self.bn = nn.BatchNorm1d(in_channels * A_size[1])
                    self.stgc1 = STGC_block(in_channels, 32, 1, t_kernel_size, A_size, dropout=0.1)
                    self.stgc2 = STGC_block(32, 32, 1, t_kernel_size, A_size, dropout=0.1)
                    self.stgc3 = STGC_block(32, 32, 1, t_kernel_size, A_size, dropout=0.1)
                    self.stgc4 = STGC_block(32, 64, 2, t_kernel_size, A_size, dropout=0.1)
                    self.stgc5 = STGC_block(64, 64, 1, t_kernel_size, A_size, dropout=0.1)
                    self.stgc6 = STGC_block(64, 64, 1, t_kernel_size, A_size, dropout=0.1)
                    self.fc = nn.Sequential(
                        nn.Linear(64, 32),
                        nn.ReLU(),
                        nn.Dropout(0.5),
                        nn.Linear(32, num_classes)
                    )
                
                def forward(self, x):
                    N, C, T, V = x.size()
                    x = x.permute(0, 3, 1, 2).contiguous().view(N, V * C, T)
                    x = self.bn(x)
                    x = x.view(N, V, C, T).permute(0, 2, 3, 1).contiguous()
                    x = self.stgc1(x, self.A)
                    x = self.stgc2(x, self.A)
                    x = self.stgc3(x, self.A)
                    x = self.stgc4(x, self.A)
                    x = self.stgc5(x, self.A)
                    x = self.stgc6(x, self.A)
                    x = F.avg_pool2d(x, x.size()[2:])
                    x = x.view(N, -1)
                    x = self.fc(x)
                    return x
        
        # åˆ›å»ºæ¨¡å‹å®ä¾‹
        model = COCO_ST_GCN(**model_config).to(DEVICE)
        
        # åŠ è½½æƒé‡
        model.load_state_dict(checkpoint['model_state_dict'])
        
        # ä»checkpointè¯»å–å…¶ä»–ä¿¡æ¯
        epoch = checkpoint.get('epoch', 'æœªçŸ¥')
        val_acc = checkpoint.get('val_acc', 'æœªçŸ¥')
        
        print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼")
        print(f"   è®­ç»ƒè½®æ¬¡: {epoch}")
        print(f"   éªŒè¯å‡†ç¡®ç‡: {val_acc}%")
        
    else:
        # å¦‚æœcheckpointä¸­æ²¡æœ‰model_state_dictï¼Œå‡è®¾æ•´ä¸ªæ–‡ä»¶å°±æ˜¯æ¨¡å‹
        print("âš ï¸ Checkpointä¸­æ²¡æœ‰model_state_dictï¼Œå°è¯•ç›´æ¥åŠ è½½ä¸ºæ¨¡å‹")
        
        # è¿™é‡Œéœ€è¦æ ¹æ®ä½ çš„å®é™…æ¨¡å‹ç»“æ„æ¥è°ƒæ•´
        # æœ€ç®€å•çš„æ–¹æ³•ï¼šä»è®­ç»ƒä»£ç å¯¼å…¥
        try:
            from ST_GCN import COCO_ST_GCN
            # éœ€è¦çŸ¥é“æ¨¡å‹å‚æ•°ï¼Œè¿™é‡Œä½¿ç”¨é»˜è®¤å€¼
            model = COCO_ST_GCN(
                num_classes=2,
                in_channels=2,
                t_kernel_size=3,
                hop_size=2
            ).to(DEVICE)
            model.load_state_dict(checkpoint)
            print("âœ… ç›´æ¥åŠ è½½æ¨¡å‹æƒé‡æˆåŠŸ")
        except:
            print("âŒ æ— æ³•åŠ è½½æ¨¡å‹ï¼Œè¯·æ£€æŸ¥æ¨¡å‹ç»“æ„æ˜¯å¦åŒ¹é…")
            return None
    
    # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    model.eval()
    print("âœ… æ¨¡å‹å·²åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼")
    
    return model

# -------------------------- 2. å…³é”®ç‚¹æå–å·¥å…· --------------------------
# åˆå§‹åŒ–YOLOå§¿æ€æ¨¡å‹
try:
    yolo_pose = YOLO("weights/yolo11m-pose.pt")  # æ ¹æ®ä½ çš„è·¯å¾„è°ƒæ•´
    print("âœ… YOLOå§¿æ€æ¨¡å‹åŠ è½½æˆåŠŸ")
except:
    print("âš ï¸ æ— æ³•åŠ è½½YOLOæ¨¡å‹ï¼Œè¯·æ£€æŸ¥è·¯å¾„")
    yolo_pose = None

def extract_pose_from_frame(frame, normalize=True):
    """
    ä»å•å¸§æå–å§¿æ€å…³é”®ç‚¹
    """
    if yolo_pose is None:
        print("âŒ YOLOæ¨¡å‹æœªåŠ è½½")
        return np.zeros(34)
    
    h, w = frame.shape[:2]
    results = yolo_pose(frame, conf=CONF_THRESHOLD)
    
    norm_pose = np.zeros((17, 2))
    
    if len(results[0].keypoints) > 0:
        kpts = results[0].keypoints.data[0].cpu().numpy()
        for i in range(17):
            x, y, conf = kpts[i]
            if conf >= CONF_THRESHOLD:
                if normalize:
                    norm_pose[i] = [x / w, y / h]
                else:
                    norm_pose[i] = [x, y]
    
    return norm_pose.flatten()

# -------------------------- 3. æ—¶åºçª—å£æ„å»º --------------------------
class PoseWindowBuffer:
    def __init__(self, window_size=WINDOW_SIZE):
        self.window_size = window_size
        self.buffer = []
    
    def add_pose(self, pose):
        self.buffer.append(pose)
        if len(self.buffer) > self.window_size:
            self.buffer.pop(0)
    
    def get_window(self):
        if len(self.buffer) < self.window_size:
            pad_len = self.window_size - len(self.buffer)
            pad_pose = np.zeros((pad_len, 34))
            window = np.concatenate([pad_pose, np.array(self.buffer)], axis=0)
        else:
            window = np.array(self.buffer)
        
        # è½¬æ¢ä¸ºSTGCNæ ¼å¼: [C, T, V] = [2, WINDOW_SIZE, 17]
        T, total_dims = window.shape
        V = 17
        C = total_dims // V
        
        window_reshaped = window.reshape(T, V, C).transpose(2, 0, 1)
        window_reshaped = window_reshaped[np.newaxis, :, :, :]
        
        return window_reshaped

# -------------------------- 4. æ ¸å¿ƒæ¨ç†å‡½æ•° --------------------------
def predict_frame_sequence(model, pose_window):
    """å¯¹æ—¶åºå§¿æ€çª—å£è¿›è¡Œé¢„æµ‹"""
    pose_tensor = torch.tensor(pose_window, dtype=torch.float32).to(DEVICE)
    
    with torch.no_grad():
        output = model(pose_tensor)
        probabilities = F.softmax(output, dim=1)
        pred_prob = probabilities[0, 1].cpu().numpy()
    
    pred_label = 1 if pred_prob > PRED_THRESHOLD else 0
    
    return pred_prob, pred_label

# -------------------------- 5. å¯è§†åŒ–ç»˜åˆ¶ --------------------------
def draw_pred_result(frame, pred_prob, pred_label):
    """åœ¨å¸§ä¸Šç»˜åˆ¶é¢„æµ‹ç»“æœ"""
    h, w = frame.shape[:2]
    
    if pred_label == 1:
        bg_color = (0, 0, 255)
        text = f"Abnromal: {pred_prob:.3f}"
    else:
        bg_color = (0, 255, 0)
        text = f"Normal: {pred_prob:.3f}"
    
    # ç»˜åˆ¶çŠ¶æ€æ–‡å­—
    cv2.rectangle(frame, (10, 10), (250, 60), bg_color, -1)
    cv2.putText(frame, text, (20, 45), 
                cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
    
    return frame

# -------------------------- 6. ç®€åŒ–ç‰ˆæœ¬ï¼šåªåšè§†é¢‘æ¨ç† --------------------------
def infer_video_simple(model_path, video_path, save_output=False):
    """
    ç®€åŒ–ç‰ˆæœ¬ï¼šç›´æ¥åŠ è½½æ¨¡å‹å¹¶è¿›è¡Œè§†é¢‘æ¨ç†
    """
    print("="*60)
    print("COCO-STGCNè§†é¢‘æ¨ç†å·¥å…·")
    print("="*60)
    
    # åŠ è½½æ¨¡å‹
    model = load_trained_model(model_path)
    if model is None:
        print("âŒ æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œç¨‹åºé€€å‡º")
        return
    
    # æ£€æŸ¥è§†é¢‘æ–‡ä»¶
    if not os.path.exists(video_path):
        print(f"âŒ è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨ï¼š{video_path}")
        return
    
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        print(f"âŒ æ— æ³•æ‰“å¼€è§†é¢‘ï¼š{video_path}")
        return
    
    # è·å–è§†é¢‘å‚æ•°
    fps = int(cap.get(cv2.CAP_PROP_FPS))
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    
    print(f"ğŸ“Š è§†é¢‘ä¿¡æ¯:")
    print(f"  å°ºå¯¸: {width}x{height}, FPS: {fps}, æ€»å¸§æ•°: {total_frames}")
    
    # ä¿å­˜è¾“å‡º
    if save_output:
        output_path = os.path.splitext(video_path)[0] + "_result.mp4"
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out_writer = cv2.VideoWriter(output_path, fourcc, fps, (width, height))
    
    # åˆå§‹åŒ–ç¼“å†²åŒº
    pose_buffer = PoseWindowBuffer(WINDOW_SIZE)
    
    print(f"\nğŸš€ å¼€å§‹æ¨ç†... (æŒ‰ESCé”®é€€å‡º)")
    
    frame_count = 0
    abnormal_count = 0
    
    while True:
        ret, frame = cap.read()
        if not ret:
            break
        
        frame_count += 1
        
        # æå–å§¿æ€
        pose = extract_pose_from_frame(frame, normalize=True)
        pose_buffer.add_pose(pose)
        
        # è‡³å°‘æ”¶é›†äº†1å¸§æ‰å¼€å§‹é¢„æµ‹
        if frame_count >= 1:
            pose_window = pose_buffer.get_window()
            pred_prob, pred_label = predict_frame_sequence(model, pose_window)
            
            if pred_label == 1:
                abnormal_count += 1
            
            # ç»˜åˆ¶ç»“æœ
            frame = draw_pred_result(frame, pred_prob, pred_label)
            
            # æ˜¾ç¤ºè¿›åº¦
            progress = frame_count / total_frames * 100
            cv2.putText(frame, f"PROCESS: {progress:.1f}%", 
                        (10, height-30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
            
            # æ˜¾ç¤ºå¸§æ•°
            cv2.putText(frame, f"FRAME: {frame_count}", 
                        (10, height-60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
        
        # æ˜¾ç¤º
        cv2.imshow("STGCNå¼‚å¸¸æ£€æµ‹", frame)
        
        # ä¿å­˜
        if save_output:
            out_writer.write(frame)
        
        # è¿›åº¦æ˜¾ç¤º
        if frame_count % 30 == 0:
            print(f"å¤„ç†: {frame_count}/{total_frames} ({progress:.1f}%)")
        
        # ESCé€€å‡º
        if cv2.waitKey(1) & 0xFF == 27:
            print("âš ï¸ ç”¨æˆ·æå‰é€€å‡º")
            break
    
    # æ¸…ç†
    cap.release()
    if save_output:
        out_writer.release()
        print(f"âœ… ç»“æœä¿å­˜åˆ°: {output_path}")
    
    cv2.destroyAllWindows()
    
    # ç»Ÿè®¡
    print(f"\nğŸ“Š æ¨ç†ç»Ÿè®¡:")
    print(f"æ€»å¸§æ•°: {frame_count}")
    print(f"å¼‚å¸¸å¸§æ•°: {abnormal_count}")
    if frame_count > 0:
        print(f"å¼‚å¸¸ç‡: {abnormal_count/frame_count*100:.2f}%")

# -------------------------- ä¸»å‡½æ•° --------------------------
if __name__ == "__main__":
    # ä½¿ç”¨ç¤ºä¾‹
    model_path = "model/best_model.pth"
    video_path = "video_origin//run_woman2.mp4"  # ä½ çš„è§†é¢‘è·¯å¾„
    
    # ç›´æ¥è°ƒç”¨ç®€åŒ–ç‰ˆæœ¬
    infer_video_simple(
        model_path=model_path,
        video_path=video_path,
        save_output=True  # æ˜¯å¦ä¿å­˜ç»“æœè§†é¢‘
    )