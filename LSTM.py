import os
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler
import torch.nn.functional as F
from tqdm import tqdm
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc
from sklearn.utils.class_weight import compute_class_weight
from matplotlib import rcParams #å­—ä½“
rcParams['font.family'] = 'SimHei'

# ============================ é…ç½®é¡¹ ============================
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ä½¿ç”¨è®¾å¤‡ï¼š{DEVICE}")

DATASET_DIR = "video_dataset"
MODEL_SAVE_PATH = os.path.join("model", "running_anomaly_lstm_pytorch.pth")
EPOCHS = 50
BATCH_SIZE = 32
LEARNING_RATE = 0.001
WEIGHT_DECAY = 1e-5
PATIENCE = 8
FACTOR = 0.5
MIN_LR = 1e-6
INPUT_DIM = 34
HIDDEN_DIM = 64
NUM_LAYERS = 2
DROPOUT = 0.2
THRESHOLD = 0.4 #æ£€æµ‹é˜ˆå€¼
# =================================================================

# -------------------------- 1. ä¿®å¤æ•°æ®é›†ç±» --------------------------
class RunningPoseDataset(Dataset):
    def __init__(self, X, y):
        self.X = torch.tensor(X, dtype=torch.float32)
        self.y = torch.tensor(y, dtype=torch.float32).unsqueeze(1)  # (N,1)
    
    def __len__(self):
        return len(self.X)
    
    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]

# -------------------------- 2. ä¿®å¤æ•°æ®åŠ è½½ï¼ˆç±»åˆ«æƒé‡å¤„ç†ï¼‰ --------------------------
def load_dataset():
    # åŠ è½½æ•°æ®
    train_data = np.load(os.path.join(DATASET_DIR, "train.npz"))
    X_train = train_data["X"]
    y_train = train_data["y"]
    
    test_data = np.load(os.path.join(DATASET_DIR, "test.npz"))
    X_test = test_data["X"]
    y_test = test_data["y"]
    
    print("âœ… æ•°æ®é›†åŠ è½½å®Œæˆï¼š")
    print(f"   è®­ç»ƒé›†ï¼šX={X_train.shape}, y={y_train.shape}")
    print(f"   æµ‹è¯•é›†ï¼šX={X_test.shape}, y={y_test.shape}")
    
    # æ„å»ºDataset
    train_dataset = RunningPoseDataset(X_train, y_train)
    test_dataset = RunningPoseDataset(X_test, y_test)
    
    # ä¿®å¤ï¼šè®¡ç®—ç±»åˆ«æƒé‡ï¼ˆç”¨äºWeightedRandomSamplerï¼Œè€ŒéLossï¼‰
    class_weights_np = compute_class_weight(
        class_weight="balanced",
        classes=np.unique(y_train),
        y=y_train
    )
    # ç”Ÿæˆæ¯ä¸ªæ ·æœ¬çš„æƒé‡ï¼ˆç”¨äºé‡‡æ ·ï¼‰
    sample_weights = np.array([class_weights_np[int(label)] for label in y_train])
    sample_weights = torch.tensor(sample_weights, dtype=torch.float32)
    
    # åŠ æƒé‡‡æ ·å™¨ï¼ˆå¹³è¡¡è®­ç»ƒé›†ç±»åˆ«ï¼‰
    sampler = WeightedRandomSampler(
        weights=sample_weights,
        num_samples=len(sample_weights),
        replacement=True
    )
    
    # DataLoader
    train_loader = DataLoader(
        train_dataset,
        batch_size=BATCH_SIZE,
        sampler=sampler,
        shuffle=False
    )
    test_loader = DataLoader(
        test_dataset,
        batch_size=BATCH_SIZE,
        shuffle=False
    )
    
    return train_loader, test_loader

# -------------------------- 3. ä¿®å¤LSTMæ¨¡å‹ï¼ˆæ— æ”¹åŠ¨ï¼‰ --------------------------
class RunningAnomalyLSTM(nn.Module):
    def __init__(self, input_dim, hidden_dim, num_layers, dropout):
        super(RunningAnomalyLSTM, self).__init__()
        self.lstm = nn.LSTM(
            input_size=input_dim,
            hidden_size=hidden_dim,
            num_layers=num_layers,
            batch_first=True,
            dropout=dropout if num_layers > 1 else 0
        )
        self.batch_norm1 = nn.BatchNorm1d(hidden_dim)
        self.fc1 = nn.Linear(hidden_dim, 32)
        self.fc2 = nn.Linear(32, 16)
        self.fc3 = nn.Linear(16, 1)
        self.dropout = nn.Dropout(dropout)
        self.relu = nn.ReLU()
    
    def forward(self, x):
        lstm_out, (hn, cn) = self.lstm(x)
        out = lstm_out[:, -1, :]  # å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥
        out = self.batch_norm1(out)
        out = self.dropout(out)
        out = self.relu(self.fc1(out))
        out = self.dropout(out)
        out = self.relu(self.fc2(out))
        out = torch.sigmoid(self.fc3(out))
        return out

# -------------------------- 4. ä¿®å¤è®­ç»ƒå‡½æ•°ï¼ˆLossæƒé‡é—®é¢˜ï¼‰ --------------------------
def train_model(model, train_loader, test_loader):
    # ä¿®å¤ï¼šBCELossä¸ä¼ å…¥class_weightsï¼ˆæƒé‡å·²é€šè¿‡é‡‡æ ·å™¨å¹³è¡¡ï¼‰
    criterion = nn.BCELoss()
    optimizer = optim.Adam(
        model.parameters(),
        lr=LEARNING_RATE,
        weight_decay=WEIGHT_DECAY
    )
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer,
        mode="min",
        factor=FACTOR,
        patience=4,
        min_lr=MIN_LR,
        verbose=True
    )
    
    best_val_loss = float("inf")
    patience_counter = 0
    train_loss_history = []
    val_loss_history = []
    train_acc_history = []
    val_acc_history = []
    
    print("\nğŸš€ å¼€å§‹è®­ç»ƒLSTMæ¨¡å‹ï¼ˆPyTorchï¼‰...")
    for epoch in range(EPOCHS):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_loss = 0.0
        train_correct = 0
        train_total = 0
        
        for batch_idx, (X_batch, y_batch) in enumerate(tqdm(train_loader, desc=f"Epoch {epoch+1}/{EPOCHS}")):
            X_batch = X_batch.to(DEVICE)
            y_batch = y_batch.to(DEVICE)
            
            optimizer.zero_grad()
            outputs = model(X_batch)
            
            # ä¿®å¤ï¼šè¾“å‡ºå’Œæ ‡ç­¾ç»´åº¦åŒ¹é…ï¼ˆå‡ä¸º[N,1]ï¼‰
            loss = criterion(outputs, y_batch)
            
            loss.backward()
            optimizer.step()
            
            train_loss += loss.item() * X_batch.size(0)
            pred = (outputs > 0.5).float()
            train_correct += (pred == y_batch).sum().item()
            train_total += y_batch.size(0)
        
        # éªŒè¯é˜¶æ®µ
        model.eval()
        val_loss = 0.0
        val_correct = 0
        val_total = 0
        
        with torch.no_grad():
            for X_batch, y_batch in test_loader:
                X_batch = X_batch.to(DEVICE)
                y_batch = y_batch.to(DEVICE)
                
                outputs = model(X_batch)
                loss = criterion(outputs, y_batch)
                
                val_loss += loss.item() * X_batch.size(0)
                pred = (outputs > 0.5).float()
                val_correct += (pred == y_batch).sum().item()
                val_total += y_batch.size(0)
        
        # è®¡ç®—å¹³å‡æŒ‡æ ‡
        avg_train_loss = train_loss / train_total
        avg_val_loss = val_loss / val_total
        train_acc = train_correct / train_total
        val_acc = val_correct / val_total
        
        # è®°å½•å†å²
        train_loss_history.append(avg_train_loss)
        val_loss_history.append(avg_val_loss)
        train_acc_history.append(train_acc)
        val_acc_history.append(val_acc)
        
        # å­¦ä¹ ç‡è°ƒåº¦
        scheduler.step(avg_val_loss)
        
        # æ‰“å°ç»“æœ
        print(f"\nEpoch {epoch+1} ç»“æœï¼š")
        print(f"è®­ç»ƒæŸå¤±ï¼š{avg_train_loss:.4f} | è®­ç»ƒå‡†ç¡®ç‡ï¼š{train_acc:.4f}")
        print(f"éªŒè¯æŸå¤±ï¼š{avg_val_loss:.4f} | éªŒè¯å‡†ç¡®ç‡ï¼š{val_acc:.4f}")
        
        # æ—©åœé€»è¾‘
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            patience_counter = 0
            # ä¿å­˜æ¨¡å‹
            torch.save({
                'epoch': epoch+1,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'best_loss': best_val_loss,
            }, MODEL_SAVE_PATH)
            print(f"âœ… ä¿å­˜æœ€ä¼˜æ¨¡å‹ï¼ˆéªŒè¯æŸå¤±ï¼š{best_val_loss:.4f}ï¼‰")
        else:
            patience_counter += 1
            print(f"âš ï¸ æ—©åœè®¡æ•°å™¨ï¼š{patience_counter}/{PATIENCE}")
            if patience_counter >= PATIENCE:
                print("ğŸ›‘ éªŒè¯æŸå¤±ä¸å†ä¸‹é™ï¼Œè§¦å‘æ—©åœ")
                break
    
    # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
    plot_training_history(train_loss_history, val_loss_history, train_acc_history, val_acc_history)
    
    return model

# -------------------------- 5. è¯„ä¼°/å¯è§†åŒ–/æ¨ç†ï¼ˆæ— æ”¹åŠ¨ï¼‰ --------------------------
def evaluate_model(model, test_loader):
    model.eval()
    all_preds = []
    all_labels = []
    all_probs = []
    
    with torch.no_grad():
        for X_batch, y_batch in test_loader:
            X_batch = X_batch.to(DEVICE)
            y_batch = y_batch.to(DEVICE)
            
            outputs = model(X_batch)
            probs = outputs.cpu().numpy().flatten()
            preds = (outputs > 0.5).float().cpu().numpy().flatten()
            labels = y_batch.cpu().numpy().flatten()
            
            all_probs.extend(probs)
            all_preds.extend(preds)
            all_labels.extend(labels)
    
    all_preds = np.array(all_preds)
    all_labels = np.array(all_labels)
    all_probs = np.array(all_probs)
    
    print("\nğŸ“Š æ¨¡å‹è¯„ä¼°ç»“æœï¼ˆæµ‹è¯•é›†ï¼‰ï¼š")
    print(classification_report(
        all_labels,
        all_preds,
        target_names=["æ­£å¸¸(0)", "å¼‚å¸¸(1)"],
        digits=4
    ))
    
    cm = confusion_matrix(all_labels, all_preds)
    print("\nğŸ” æ··æ·†çŸ©é˜µï¼š")
    print(f"          é¢„æµ‹æ­£å¸¸  é¢„æµ‹å¼‚å¸¸")
    print(f"å®é™…æ­£å¸¸   {cm[0][0]}        {cm[0][1]}")
    print(f"å®é™…å¼‚å¸¸   {cm[1][0]}        {cm[1][1]}")
    
    fpr, tpr, _ = roc_curve(all_labels, all_probs)
    roc_auc = auc(fpr, tpr)
    print(f"\nğŸ“ˆ AUCå€¼ï¼š{roc_auc:.4f}")
    
    plt.figure(figsize=(8, 6))
    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROCæ›²çº¿ (AUC = {roc_auc:.4f})')
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('å‡é˜³æ€§ç‡ï¼ˆFPRï¼‰')
    plt.ylabel('çœŸé˜³æ€§ç‡ï¼ˆTPRï¼‰')
    plt.title('ROCæ›²çº¿')
    plt.legend(loc="lower right")
    plt.savefig(os.path.join(DATASET_DIR, "roc_curve.png"))
    plt.close()
    
    return all_preds, all_probs

def plot_training_history(train_loss, val_loss, train_acc, val_acc):
    plt.figure(figsize=(12, 5))
    
    plt.subplot(1, 2, 1)
    plt.plot(train_loss, label='è®­ç»ƒæŸå¤±', color='blue')
    plt.plot(val_loss, label='éªŒè¯æŸå¤±', color='red')
    plt.title('æŸå¤±æ›²çº¿')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid(True)
    
    plt.subplot(1, 2, 2)
    plt.plot(train_acc, label='è®­ç»ƒå‡†ç¡®ç‡', color='blue')
    plt.plot(val_acc, label='éªŒè¯å‡†ç¡®ç‡', color='red')
    plt.title('å‡†ç¡®ç‡æ›²çº¿')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.grid(True)
    
    plt.tight_layout()
    plt.savefig(os.path.join(DATASET_DIR, "training_history.png"))
    plt.close()
    print(f"\nâœ… è®­ç»ƒæ›²çº¿å·²ä¿å­˜è‡³ï¼š{os.path.join(DATASET_DIR, 'training_history.png')}")

def predict_new_data(model_path, new_X):
    model = RunningAnomalyLSTM(INPUT_DIM, HIDDEN_DIM, NUM_LAYERS, DROPOUT).to(DEVICE)
    checkpoint = torch.load(model_path, map_location=DEVICE)
    model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()
    
    new_X_tensor = torch.tensor(new_X, dtype=torch.float32).to(DEVICE)
    
    with torch.no_grad():
        outputs = model(new_X_tensor)
        probs = outputs.cpu().numpy().flatten()
        preds = (outputs > THRESHOLD).float().cpu().numpy().flatten()
    
    print("\nğŸ”® æ–°æ•°æ®é¢„æµ‹ç»“æœï¼š")
    for i in range(len(preds)):
        print(f"æ ·æœ¬{i}ï¼šå¼‚å¸¸æ¦‚ç‡={probs[i]:.4f} â†’ {'å¼‚å¸¸(1)' if preds[i]==1 else 'æ­£å¸¸(0)'}")
    
    return probs, preds

# -------------------------- ä¸»å‡½æ•° --------------------------
if __name__ == "__main__":
    # åŠ è½½æ•°æ®
    train_loader, test_loader = load_dataset()
    
    # åˆå§‹åŒ–æ¨¡å‹
    model = RunningAnomalyLSTM(
        input_dim=INPUT_DIM,
        hidden_dim=HIDDEN_DIM,
        num_layers=NUM_LAYERS,
        dropout=DROPOUT
    ).to(DEVICE)
    print("\nğŸ“Œ LSTMæ¨¡å‹ç»“æ„ï¼š")
    print(model)
    
    # è®­ç»ƒæ¨¡å‹
    model = train_model(model, train_loader, test_loader)
    
    # åŠ è½½æœ€ä¼˜æ¨¡å‹è¯„ä¼°
    checkpoint = torch.load(MODEL_SAVE_PATH, map_location=DEVICE)
    model.load_state_dict(checkpoint['model_state_dict'])
    print(f"\nğŸ“Œ åŠ è½½æœ€ä¼˜æ¨¡å‹ï¼ˆEpoch {checkpoint['epoch']}ï¼ŒéªŒè¯æŸå¤± {checkpoint['best_loss']:.4f}ï¼‰")
    evaluate_model(model, test_loader)
    
    # é¢„æµ‹ç¤ºä¾‹
    print("\n==================== é¢„æµ‹ç¤ºä¾‹ ====================")
    test_data = np.load(os.path.join(DATASET_DIR, "test.npz"))
    X_test = test_data["X"]
    sample_X = X_test[:10]
    predict_new_data(MODEL_SAVE_PATH, sample_X)
    
    print(f"\nğŸ‰ è®­ç»ƒå®Œæˆï¼æ¨¡å‹ä¿å­˜è‡³ï¼š{MODEL_SAVE_PATH}")