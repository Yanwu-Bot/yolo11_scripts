import os
import cv2
import numpy as np
import torch
import torch.nn as nn
from ultralytics import YOLO
from matplotlib import rcParams #å­—ä½“
rcParams['font.family'] = 'SimHei'

# ============================ é…ç½®é¡¹ï¼ˆéœ€ä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰ ============================
# è®¾å¤‡é…ç½®
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# æ¨¡å‹è·¯å¾„
MODEL_PATH = "model\\running_anomaly_lstm_pytorch.pth"
# æ¨¡å‹å‚æ•°ï¼ˆå¿…é¡»å’Œè®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´ï¼‰
INPUT_DIM = 34       # 17å…³èŠ‚Ã—2åæ ‡
HIDDEN_DIM = 64      # LSTMéšè—å±‚ç»´åº¦
NUM_LAYERS = 2       # LSTMå±‚æ•°
DROPOUT = 0.2        # Dropoutæ¯”ä¾‹
# æ—¶åºçª—å£å‚æ•°ï¼ˆå¿…é¡»å’Œè®­ç»ƒæ—¶ä¸€è‡´ï¼‰
WINDOW_SIZE = 12     # LSTMæ—¶é—´æ­¥é•¿ï¼ˆè¿ç»­12å¸§ï¼‰
STEP = 4             # çª—å£æ»‘åŠ¨æ­¥é•¿ï¼ˆæ¨ç†æ—¶å¯è®¾ä¸º1ï¼Œå®æ—¶æ€§æ›´é«˜ï¼‰
# å…³é”®ç‚¹æå–å‚æ•°
CONF_THRESHOLD = 0.5 # YOLOå…³é”®ç‚¹ç½®ä¿¡åº¦é˜ˆå€¼
# é¢„æµ‹é˜ˆå€¼ï¼ˆå¯è°ƒæ•´ï¼šè¶Šä½è¶Šçµæ•ï¼Œè¶Šé«˜è¶Šä¸¥æ ¼ï¼‰
PRED_THRESHOLD = 0.3 # æ¦‚ç‡>é˜ˆå€¼åˆ¤å®šä¸ºå¼‚å¸¸
# ==================================================================================

# -------------------------- 1. å®šä¹‰LSTMæ¨¡å‹ï¼ˆä¸è®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´ï¼‰ --------------------------
class RunningAnomalyLSTM(nn.Module):
    def __init__(self, input_dim, hidden_dim, num_layers, dropout):
        super(RunningAnomalyLSTM, self).__init__()
        self.lstm = nn.LSTM(
            input_size=input_dim,
            hidden_size=hidden_dim,
            num_layers=num_layers,
            batch_first=True,
            dropout=dropout if num_layers > 1 else 0
        )
        self.batch_norm1 = nn.BatchNorm1d(hidden_dim)
        self.fc1 = nn.Linear(hidden_dim, 32)
        self.fc2 = nn.Linear(32, 16)
        self.fc3 = nn.Linear(16, 1)
        self.dropout = nn.Dropout(dropout)
        self.relu = nn.ReLU()
    
    def forward(self, x):
        lstm_out, (hn, cn) = self.lstm(x)
        out = lstm_out[:, -1, :]  # å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥è¾“å‡º
        out = self.batch_norm1(out)
        out = self.dropout(out)
        out = self.relu(self.fc1(out))
        out = self.dropout(out)
        out = self.relu(self.fc2(out))
        out = torch.sigmoid(self.fc3(out))
        return out

# -------------------------- 2. åŠ è½½LSTMæ¨¡å‹ --------------------------
def load_lstm_model(model_path):
    """åŠ è½½è®­ç»ƒå¥½çš„LSTMæ¨¡å‹"""
    # åˆå§‹åŒ–æ¨¡å‹
    model = RunningAnomalyLSTM(INPUT_DIM, HIDDEN_DIM, NUM_LAYERS, DROPOUT).to(DEVICE)
    # åŠ è½½æƒé‡ï¼ˆå¿½ç•¥ä¼˜åŒ–å™¨ç­‰æ— å…³å‚æ•°ï¼‰
    checkpoint = torch.load(model_path, map_location=DEVICE)
    model.load_state_dict(checkpoint['model_state_dict'])
    # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼ï¼ˆç¦ç”¨Dropout/BatchNormè®­ç»ƒè¡Œä¸ºï¼‰
    model.eval()
    print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼š{model_path}")
    return model

# -------------------------- 3. å…³é”®ç‚¹æå–å·¥å…·ï¼ˆä¸æ•°æ®é›†ç”Ÿæˆä¸€è‡´ï¼‰ --------------------------
# åˆå§‹åŒ–YOLOå§¿æ€æ¨¡å‹
yolo_pose = YOLO("yolo11m-pose.pt")

def extract_pose_from_frame(frame, normalize=True):
    """
    ä»å•å¸§æå–å½’ä¸€åŒ–çš„å§¿æ€å…³é”®ç‚¹
    :param frame: åŸå§‹è§†é¢‘å¸§ (H,W,3)
    :param normalize: æ˜¯å¦å½’ä¸€åŒ–ï¼ˆå¿…é¡»å’Œè®­ç»ƒæ—¶ä¸€è‡´ï¼‰
    :return: norm_pose (34,) â†’ å½’ä¸€åŒ–åçš„å…³é”®ç‚¹ï¼Œæ— å…³é”®ç‚¹åˆ™è¿”å›å…¨0
    """
    h, w = frame.shape[:2]
    results = yolo_pose(frame, conf=CONF_THRESHOLD)
    
    # åˆå§‹åŒ–å…³é”®ç‚¹
    norm_pose = np.zeros((17, 2))  # (17å…³èŠ‚, x/y)
    
    if len(results[0].keypoints) > 0:
        kpts = results[0].keypoints.data[0].cpu().numpy()  # (17, 3) x,y,conf
        for i in range(17):
            x, y, conf = kpts[i]
            if conf >= CONF_THRESHOLD:
                # åŸå§‹åƒç´ åæ ‡
                raw_x, raw_y = x, y
                # å½’ä¸€åŒ–ï¼ˆå’Œè®­ç»ƒæ—¶çš„é¢„å¤„ç†é€»è¾‘å®Œå…¨ä¸€è‡´ï¼‰
                if normalize:
                    # åŸºäºäººä½“åŒ…å›´ç›’å½’ä¸€åŒ–
                    non_zero_kpts = kpts[kpts[:,2]>=CONF_THRESHOLD, :2]
                    if len(non_zero_kpts) > 0:
                        min_xy = np.min(non_zero_kpts, axis=0)
                        max_xy = np.max(non_zero_kpts, axis=0)
                        bbox_w = max_xy[0] - min_xy[0] if max_xy[0] > min_xy[0] else 1
                        bbox_h = max_xy[1] - min_xy[1] if max_xy[1] > min_xy[1] else 1
                        norm_x = (raw_x - min_xy[0]) / bbox_w
                        norm_y = (raw_y - min_xy[1]) / bbox_h
                        norm_pose[i] = [norm_x, norm_y]
                else:
                    norm_pose[i] = [raw_x/w, raw_y/h]  # åŸºäºç”»é¢å½’ä¸€åŒ–
    
    return norm_pose.flatten()  # (34,)

# -------------------------- 4. æ—¶åºçª—å£æ„å»ºï¼ˆæ ¸å¿ƒï¼šåŒ¹é…LSTMè¾“å…¥ï¼‰ --------------------------
class PoseWindowBuffer:
    """å§¿æ€çª—å£ç¼“å†²åŒºï¼šç»´æŠ¤æœ€è¿‘Nå¸§çš„å§¿æ€ï¼Œæ„å»ºLSTMè¾“å…¥çš„æ—¶åºçª—å£"""
    def __init__(self, window_size=WINDOW_SIZE):
        self.window_size = window_size
        self.buffer = []  # å­˜å‚¨æœ€è¿‘çš„å§¿æ€åºåˆ—
    
    def add_pose(self, pose):
        """æ·»åŠ å•å¸§å§¿æ€åˆ°ç¼“å†²åŒº"""
        self.buffer.append(pose)
        # ä¿æŒç¼“å†²åŒºé•¿åº¦ä¸è¶…è¿‡çª—å£å¤§å°
        if len(self.buffer) > self.window_size:
            self.buffer.pop(0)
    
    def get_window(self):
        """è·å–å®Œæ•´çš„æ—¶åºçª—å£ï¼ˆä¸è¶³åˆ™è¡¥0ï¼‰"""
        if len(self.buffer) < self.window_size:
            # ä¸è¶³çª—å£å¤§å°ï¼šå‰é¢è¡¥0
            pad_len = self.window_size - len(self.buffer)
            pad_pose = np.zeros((pad_len, INPUT_DIM))
            window = np.concatenate([pad_pose, np.array(self.buffer)], axis=0)
        else:
            window = np.array(self.buffer)
        return window[np.newaxis, :, :]  # å¢åŠ batchç»´åº¦ â†’ (1, window_size, 34)

# -------------------------- 5. æ ¸å¿ƒæ¨ç†å‡½æ•° --------------------------
def predict_frame_sequence(model, pose_window):
    """
    å¯¹æ—¶åºå§¿æ€çª—å£è¿›è¡Œé¢„æµ‹
    :param model: åŠ è½½å¥½çš„LSTMæ¨¡å‹
    :param pose_window: å§¿æ€çª—å£ (1, window_size, 34)
    :return: pred_prob (å¼‚å¸¸æ¦‚ç‡), pred_label (0=æ­£å¸¸/1=å¼‚å¸¸)
    """
    # è½¬æ¢ä¸ºTensorå¹¶ç§»è‡³è®¾å¤‡
    pose_tensor = torch.tensor(pose_window, dtype=torch.float32).to(DEVICE)
    
    # é¢„æµ‹ï¼ˆç¦ç”¨æ¢¯åº¦è®¡ç®—ï¼Œæå‡é€Ÿåº¦ï¼‰
    with torch.no_grad():
        pred_prob = model(pose_tensor).cpu().numpy().flatten()[0]
    pred_label = 1 if pred_prob > PRED_THRESHOLD else 0
    
    return pred_prob, pred_label

# -------------------------- 6. å¯è§†åŒ–ç»˜åˆ¶ï¼ˆå®æ—¶æ˜¾ç¤ºç»“æœï¼‰ --------------------------
def draw_pred_result(frame, pred_prob, pred_label):
    """åœ¨å¸§ä¸Šç»˜åˆ¶é¢„æµ‹ç»“æœ"""
    h, w = frame.shape[:2]
    # ç»˜åˆ¶èƒŒæ™¯æ¡†
    if pred_label == 1:  # å¼‚å¸¸ï¼šçº¢è‰²èƒŒæ™¯
        bg_color = (0, 0, 255)
        text = f"Abnormal: {pred_prob:.3f}"
    else:  # æ­£å¸¸ï¼šç»¿è‰²èƒŒæ™¯
        bg_color = (0, 255, 0)
        text = f"Normal: {pred_prob:.3f}"
    
    # ç»˜åˆ¶æ–‡å­—
    cv2.rectangle(frame, (10, 10), (300, 60), bg_color, -1)
    cv2.putText(frame, text, (20, 45), 
                cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
    return frame

# -------------------------- 7. åœºæ™¯1ï¼šå®æ—¶è§†é¢‘æµæ¨ç†ï¼ˆæ‘„åƒå¤´ï¼‰ --------------------------
def infer_realtime_camera(model):
    """å®æ—¶æ‘„åƒå¤´ç”»é¢æ¨ç†"""
    cap = cv2.VideoCapture(0)  # 0=é»˜è®¤æ‘„åƒå¤´ï¼Œå¯æ”¹ä¸ºè§†é¢‘è·¯å¾„
    if not cap.isOpened():
        print("âŒ æ— æ³•æ‰“å¼€æ‘„åƒå¤´")
        return
    
    # åˆå§‹åŒ–å§¿æ€ç¼“å†²åŒº
    pose_buffer = PoseWindowBuffer(WINDOW_SIZE)
    
    print("\nğŸš€ å®æ—¶æ¨ç†ä¸­ï¼ˆæŒ‰ESCé€€å‡ºï¼‰...")
    while True:
        ret, frame = cap.read()
        if not ret:
            break
        
        # 1. æå–å½“å‰å¸§å§¿æ€
        pose = extract_pose_from_frame(frame)
        # 2. æ·»åŠ åˆ°ç¼“å†²åŒºï¼Œæ„å»ºæ—¶åºçª—å£
        pose_buffer.add_pose(pose)
        pose_window = pose_buffer.get_window()
        # 3. é¢„æµ‹
        pred_prob, pred_label = predict_frame_sequence(model, pose_window)
        # 4. ç»˜åˆ¶ç»“æœ
        frame_with_result = draw_pred_result(frame, pred_prob, pred_label)
        # 5. æ˜¾ç¤ºç”»é¢
        cv2.imshow("Running Anomaly Detection (Real-time)", frame_with_result)
        
        # æŒ‰ESCé€€å‡º
        if cv2.waitKey(1) & 0xFF == 27:
            break
    
    cap.release()
    cv2.destroyAllWindows()

# -------------------------- 8. åœºæ™¯2ï¼šæœ¬åœ°è§†é¢‘æ–‡ä»¶æ¨ç† --------------------------
def infer_local_video(model, video_path, save_output=False):
    """
    æœ¬åœ°è§†é¢‘æ–‡ä»¶æ¨ç†
    :param video_path: è§†é¢‘è·¯å¾„
    :param save_output: æ˜¯å¦ä¿å­˜æ¨ç†ç»“æœè§†é¢‘
    """
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        print(f"âŒ æ— æ³•æ‰“å¼€è§†é¢‘ï¼š{video_path}")
        return
    
    # è·å–è§†é¢‘å‚æ•°
    fps = int(cap.get(cv2.CAP_PROP_FPS))
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    
    # ä¿å­˜è§†é¢‘é…ç½®
    if save_output:
        output_path = os.path.splitext(video_path)[0] + "_result.mp4"
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out_writer = cv2.VideoWriter(output_path, fourcc, fps, (width, height))
    
    # åˆå§‹åŒ–å§¿æ€ç¼“å†²åŒº
    pose_buffer = PoseWindowBuffer(WINDOW_SIZE)
    
    print(f"\nğŸš€ è§†é¢‘æ¨ç†ä¸­ï¼š{video_path}ï¼ˆå…±{total_frames}å¸§ï¼ŒæŒ‰ESCæå‰é€€å‡ºï¼‰...")
    for frame_idx in range(total_frames):
        ret, frame = cap.read()
        if not ret:
            break
        
        # 1. æå–å§¿æ€
        pose = extract_pose_from_frame(frame)
        # 2. æ„å»ºçª—å£
        pose_buffer.add_pose(pose)
        pose_window = pose_buffer.get_window()
        # 3. é¢„æµ‹
        pred_prob, pred_label = predict_frame_sequence(model, pose_window)
        # 4. ç»˜åˆ¶ç»“æœ
        frame_with_result = draw_pred_result(frame, pred_prob, pred_label)
        # 5. æ˜¾ç¤º
        cv2.imshow("Running Anomaly Detection (Video)", frame_with_result)
        pose = extract_pose_from_frame(frame)
        print(f"å½“å‰å¸§å§¿æ€å‰5ç»´ï¼š{pose[:5]}")  # æ‰“å°å‰5ç»´ï¼Œçœ‹æ˜¯å¦å˜åŒ–
        # 6. ä¿å­˜ï¼ˆå¯é€‰ï¼‰
        if save_output:
            out_writer.write(frame_with_result)
        
        # æŒ‰ESCé€€å‡º
        if cv2.waitKey(1) & 0xFF == 27:
            break
    
    # é‡Šæ”¾èµ„æº
    cap.release()
    if save_output:
        out_writer.release()
        print(f"âœ… æ¨ç†ç»“æœè§†é¢‘å·²ä¿å­˜ï¼š{output_path}")
    cv2.destroyAllWindows()

# -------------------------- 9. åœºæ™¯3ï¼šå•å¸§å›¾ç‰‡æ¨ç† --------------------------
def infer_single_image(model, img_path):
    """å•å¸§å›¾ç‰‡æ¨ç†ï¼ˆéœ€æ¨¡æ‹Ÿæ—¶åºçª—å£ï¼Œè¡¥å‰11å¸§ä¸º0ï¼‰"""
    frame = cv2.imread(img_path)
    if frame is None:
        print(f"âŒ æ— æ³•è¯»å–å›¾ç‰‡ï¼š{img_path}")
        return
    
    # æå–å½“å‰å¸§å§¿æ€
    pose = extract_pose_from_frame(frame)
    # æ„å»ºçª—å£ï¼ˆä»…å½“å‰å¸§æœ‰æ•ˆï¼Œå…¶ä½™è¡¥0ï¼‰
    pose_buffer = PoseWindowBuffer(WINDOW_SIZE)
    pose_buffer.add_pose(pose)
    pose_window = pose_buffer.get_window()
    # é¢„æµ‹
    pred_prob, pred_label = predict_frame_sequence(model, pose_window)
    
    # ç»˜åˆ¶ç»“æœå¹¶ä¿å­˜
    frame_with_result = draw_pred_result(frame, pred_prob, pred_label)
    output_path = os.path.splitext(img_path)[0] + "_result.jpg"
    cv2.imwrite(output_path, frame_with_result)
    
    # æ‰“å°ç»“æœ
    print("\nğŸ“Š å•å¸§æ¨ç†ç»“æœï¼š")
    print(f"å›¾ç‰‡è·¯å¾„ï¼š{img_path}")
    print(f"å¼‚å¸¸æ¦‚ç‡ï¼š{pred_prob:.4f}")
    print(f"åˆ¤å®šç»“æœï¼š{'å¼‚å¸¸' if pred_label==1 else 'æ­£å¸¸'}")
    print(f"ç»“æœä¿å­˜ï¼š{output_path}")
    
    # æ˜¾ç¤ºç»“æœ
    cv2.imshow("Single Frame Result", frame_with_result)
    cv2.waitKey(0)
    cv2.destroyAllWindows()

# -------------------------- ä¸»å‡½æ•°ï¼šé€‰æ‹©æ¨ç†åœºæ™¯ --------------------------
if __name__ == "__main__":
    # 1. åŠ è½½æ¨¡å‹
    lstm_model = load_lstm_model(MODEL_PATH)
    
    # 2. é€‰æ‹©æ¨ç†åœºæ™¯ï¼ˆå–æ¶ˆæ³¨é‡Šå¯¹åº”åœºæ™¯ï¼‰
    # åœºæ™¯1ï¼šå®æ—¶æ‘„åƒå¤´æ¨ç†
    # infer_realtime_camera(lstm_model)
    
    # åœºæ™¯2ï¼šæœ¬åœ°è§†é¢‘æ¨ç†ï¼ˆæ›¿æ¢ä¸ºä½ çš„è§†é¢‘è·¯å¾„ï¼‰
    infer_local_video(lstm_model, "video_origin\\run_woman2.mp4", save_output=True)
    
    # åœºæ™¯3ï¼šå•å¸§å›¾ç‰‡æ¨ç†ï¼ˆæ›¿æ¢ä¸ºä½ çš„å›¾ç‰‡è·¯å¾„ï¼‰
    # infer_single_image(lstm_model, "running_videos/test_frame.jpg")